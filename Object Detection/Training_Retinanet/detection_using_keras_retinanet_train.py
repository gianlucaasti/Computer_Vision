# -*- coding: utf-8 -*-
"""detection-using-keras-retinanet-train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Iuzfd2JH-Pe3WiPxc2Q2POBbBzQL1j1B

<a id="1"></a>
# <p style="background-color:#000;font-family:newtimeroman;color:#fff;font-size:120%;text-align:center;border-radius:20px 80px;">Introduction</p>

> ### RetinaNet is one of the best one-stage object detection models that has proven to work well with dense and small scale objects. For this reason, it has become a popular object detection model to be used with aerial and satellite imagery. [read more...](https://developers.arcgis.com/python/guide/how-retinanet-works/)

> ### In this notebook, I will be training RetinaNet architecture from [fizyr](https://github.com/fizyr/keras-retinanet)

<a id="1"></a>
# <p style="background-color:#000000;font-family:newtimeroman;color:#fff;font-size:120%;text-align:center;border-radius:20px 80px;">Import Libraries</p>
"""

# Commented out IPython magic to ensure Python compatibility.
# %cd "D:/Documents/GitHub/Computer_Vision/Object Detection/Training_Retinanet"

#!pip install progressbar2

#!pip install contextlib2

#!pip install shapely

# Commented out IPython magic to ensure Python compatibility.
#Clone Git Repository
#!git clone https://github.com/fizyr/keras-retinanet.git
# %cd C:/Users/Gianlu/keras-retinanet/
#!python setup.py build_ext --inplace

# Commented out IPython magic to ensure Python compatibility.
# %cd C:/Users/Gianlu/keras-retinanet/

"""**IMPORT LIBRARIES**"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
# show images inline
# %matplotlib inline

import keras
import tensorflow as tf

# import miscellaneous modules
import matplotlib.pyplot as plt
import cv2
import os
import numpy as np
import time

import ast

# import keras_retinanet
from keras_retinanet import models
from keras_retinanet.utils.image import read_image_bgr, preprocess_image, resize_image
from keras_retinanet.utils.visualization import draw_box, draw_caption
from keras_retinanet.utils.colors import label_color
from keras_retinanet import models

# import libraries to download weights
import keras_resnet
import urllib.request

# libraries to transform data
import contextlib2
import io
import IPython
import json
import pathlib
import sys

# libraries for Confusion Matrix
from shapely.geometry import Polygon,Point
import shapely
import gc

"""<a id="1"></a>
# <p style="background-color:#000000;font-family:newtimeroman;color:#fff;font-size:120%;text-align:center;border-radius:20px 80px;">Download Pretrained Weights or connect to the ones from the last training</p>

Download original weights
"""

#!pip install --upgrade git+https://github.com/broadinstitute/keras-resnet

#PRETRAINED_MODEL = './snapshots/_pretrained_model.h5'
#### OPTION 1: DOWNLOAD INITIAL PRETRAINED MODEL FROM FIZYR ####
#URL_MODEL = 'https://github.com/fizyr/keras-retinanet/releases/download/0.5.1/resnet50_coco_best_v2.1.0.h5'
#urllib.request.urlretrieve(URL_MODEL, PRETRAINED_MODEL)
#print('Downloaded pretrained model to ' + PRETRAINED_MODEL)

"""Weights from last training"""

PRETRAINED_MODEL = './snapshots/resnet50_csv_10.h5'

"""<a id="1"></a>
# <p style="background-color:#000000;font-family:newtimeroman;color:#fff;font-size:120%;text-align:center;border-radius:20px 80px;">Load Data </p>
"""

def bbox_to_dict(row):
    dictionary= dict({'x_min': ast.literal_eval(row['bbox'])[0],
                  'y_min': ast.literal_eval(row['bbox'])[1],
                  'x_max': ast.literal_eval(row['bbox'])[2],
                  'y_max': ast.literal_eval(row['bbox'])[3]}
                  )
    return dictionary

def concat_name_row(row):
    concatenation=row['img_name']+str(row['Unnamed: 0'])
    return concatenation

df_train = pd.read_csv("D:/Pictures/dataset/new_images/train_augmented.csv")

df_train=df_train.loc[df_train["class"].astype(str) != '']
#df_train['img_name'] = df_train['img_name'].apply(eval)

df_train['image_path'] = "D:/Pictures/dataset/new_images/train/" + df_train['img_name'].astype(str) 
df_extrain=df_train.explode('img_name') # Single annotation per row
df_extrain['annotations']=df_extrain.apply(lambda row: bbox_to_dict(row), axis=1)
df_extrain['img_id']=df_extrain.apply(lambda row: concat_name_row(row), axis=1)
df_extrain.reset_index(inplace=True)
df_extrain.head()

df_extrain_main=pd.DataFrame(pd.json_normalize(df_extrain['annotations']), columns=['x_min', 'y_min', 'x_max', 'y_max']).join(df_extrain)
df_extrain_main=df_extrain_main[df_extrain_main['class'].notna()]
df_extrain_main=df_extrain_main[['image_path','x_min', 'y_min', 'x_max', 'y_max','class','img_name','img_id']] 
df_extrain_main.head(10)

"""<a id="1"></a>
# <p style="background-color:#000000;font-family:newtimeroman;color:#fff;font-size:120%;text-align:center;border-radius:20px 80px;">Transfoming Data Format </p>
"""

def create_tf_example(rowss,data_df):
    """Create a tf.Example entry for a given training image."""
    full_path = os.path.join(rowss.image_path)
    with tf.io.gfile.GFile(full_path, 'rb') as fid:
        encoded_jpg = fid.read()
    encoded_jpg_io = io.BytesIO(encoded_jpg)
    image = Image.open(encoded_jpg_io)
    if image.format != 'JPEG':
        raise ValueError('Image format not JPEG')

    height = image.size[1] # Image height
    width = image.size[0] # Image width
    #print(width,height)
    filename = f'{rowss.img_id}'.encode('utf8') # Unique id of the image.
    encoded_image_data = None # Encoded image bytes
    image_format = 'jpeg'.encode('utf8') # b'jpeg' or b'png'

    xmins = [] 
    xmaxs = [] 
    ymins = [] 
    ymaxs = [] 
    
    # Convert ---> [xmin,ymin,width,height] to [xmins,xmaxs,ymins,ymaxs]
    xmin = rowss['x_min']
    xmax = rowss['x_max']
    ymin = rowss['y_min']
    ymax = rowss['y_max']
    

    #main_data.append((rowss['image_path'],xmins,xmaxs,ymins,ymaxs))
    return rowss['image_path'],xmin,ymin,xmax,ymax

tf_example1=[]

from PIL import Image, ImageDraw
for index, row in df_extrain_main.iterrows():
            if index % 1000 == 0:
                print('Processed {0} images.'.format(index))
            image_path,xmins,ymins,xmaxs,ymaxs=create_tf_example(row,df_extrain_main)
            #print(image_path,xmins,xmaxs,ymins,ymaxs)
            df_extrain_main.loc[index,'image_path']=image_path
            df_extrain_main.loc[index,'x_min']=xmins
            df_extrain_main.loc[index,'y_min']=ymins
            df_extrain_main.loc[index,'x_max']=xmaxs
            df_extrain_main.loc[index,'y_max']=ymaxs

df_extrain_main

"""<a id="1"></a>
# <p style="background-color:#000000;font-family:newtimeroman;color:#fff;font-size:120%;text-align:center;border-radius:20px 80px;">Creating CSV for Training </p>
"""

df_extrain_main['class'].unique()

classes=pd.DataFrame([{'class':'Platelets','label':0},{'class':'WBC','label':1},{'class':'RBC','label':2},{'class':'FBC','label':3}])
classes.to_csv("D:/Pictures/dataset/new_images/classes.csv",index=False,header=False)  # This CSV will be use in training

df_extrain_main['class']!=''
df_extrain_main[['image_path','x_min','y_min','x_max','y_max','class']].to_csv("D:/Pictures/dataset/new_images/annotation.csv",index=False,header=False)

"""<a id="1"></a>
# <p style="background-color:#000000;font-family:newtimeroman;color:#fff;font-size:120%;text-align:center;border-radius:20px 80px;">Training RetinaNet</p>

### Training this model 60 epochs for demo purpose.
"""

# !C:/Users/Gianlu/keras-retinanet/keras_retinanet/bin/train.py --freeze-backbone --random-transform --weights {PRETRAINED_MODEL} --batch-size 1 --steps 200 --epochs 60 csv D:/Pictures/dataset/new_images/annotation.csv D:/Pictures/dataset/new_images/classes.csv

TENSORBOARD_DIR='D:/Pictures/dataset/new_images/logs'

!C:/Users/Gianlu/keras-retinanet/keras_retinanet/bin/train.py --freeze-backbone --random-transform --weights {PRETRAINED_MODEL} --batch-size 1 --steps 200 --epochs 10  --tensorboard-dir {TENSORBOARD_DIR} --weighted-average csv D:/Pictures/dataset/new_images/annotation.csv D:/Pictures/dataset/new_images/classes.csv

#SCRIPT TO EVALUATE MODEL
!C:/Users/Gianlu/keras-retinanet/keras_retinanet/bin/evaluate.py --convert-model csv D:/Pictures/dataset/new_images/annotation.csv D:/Pictures/dataset/new_images/classes.csv {PRETRAINED_MODEL}

"""<a id="1"></a>
# <p style="background-color:#000000;font-family:newtimeroman;color:#fff;font-size:120%;text-align:center;border-radius:20px 80px;">Load Trained Model</p>
"""

model_path = os.path.join('snapshots', sorted(os.listdir('snapshots'), reverse=True)[0]) 
print(model_path)

# load retinanet model
model = models.load_model(model_path, backbone_name='resnet50')  ## Use backbone as resnet50
model = models.convert_model(model)

# load label to names mapping for visualization purposes
labels_to_names = pd.read_csv("D:/Pictures/dataset/new_images/classes.csv",header=None).T.loc[0].to_dict()

"""<a id="1"></a>
# <p style="background-color:#000000;font-family:newtimeroman;color:#fff;font-size:120%;text-align:center;border-radius:20px 80px;">Predicted vs Actual</p>
"""

THRES_SCORE = 0.4  # Set Score Threshold Value

def class_to_color(class_id):
    colors = {'Platelets':(255,0,0),'WBC':(0,255,0),'RBC':(0,0,255),'FBC':(255,255,0)}
    return colors[class_id]

def df_plot_orinal(drawOG,img_path,df):
    df=df[df['image_path']==img_path]
    for i,r in df.iterrows():
        cv2.rectangle(drawOG, (int(r['x_min']), int(r['y_min'])), (int(r['x_max']), int(r['y_max'])), (255,0,0),2)
    

def img_inference(img_path):
  image = read_image_bgr(img_path)

  # copy to draw on
  draw = image.copy()
  draw = cv2.cvtColor(draw, cv2.COLOR_BGR2RGB)
  drawOG = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
  # preprocess image for network
  image = preprocess_image(image)
  image, scale = resize_image(image)

  # process image
  start = time.time()
  boxes, scores, labels = model.predict_on_batch(np.expand_dims(image, axis=0))
  df_plot_orinal(drawOG,img_path,df_extrain_main)
  # correct for image scale
  boxes /= scale
  # visualize detections
  for box, score, label in zip(boxes[0], scores[0], labels[0]):
      # scores are sorted so we can break
      #print(score)
      if score < THRES_SCORE:
          continue
      color = label_color(label)
      b = box.astype(int)
      draw_box(draw, b, color=color)
      caption = "{} {:.3f}%".format(labels_to_names[label], score*100) #
      draw_caption(draw, b, caption)
    
  fig = plt.figure(figsize=(20, 20))
  ax1=fig.add_subplot(1, 2, 1)
  plt.imshow(draw)
  ax2=fig.add_subplot(1, 2, 2)
  plt.imshow(drawOG)

  ax1.title.set_text('Predicted')
  ax2.title.set_text('Actual')
  plt.show()

data=df_extrain_main.sample(n=5)  #Predict on Random 5 Image
for i,r in data.iterrows():
    img_inference(r['image_path'])

print(model.summary())

"""**WIP CONFUSION MATRIX with RECALL, PRECISION**"""

def img_inference_bbox_scores_labels(img_path):
  image = read_image_bgr(img_path)

  # copy to draw on
  draw = image.copy()
  draw = cv2.cvtColor(draw, cv2.COLOR_BGR2RGB)
  drawOG = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
  # preprocess image for network
  image = preprocess_image(image)
  image, scale = resize_image(image)

  # process image
  start = time.time()
  boxes, scores, labels = model.predict_on_batch(np.expand_dims(image, axis=0))
  return boxes , scores, labels

data=df_extrain_main.sample(n=5)  #Predict on Random 5 Image
selection=data['img_name'].unique()
print(selection)
for i,r in data.iterrows():
    pred_boxes=img_inference_bbox_scores_labels(r['image_path'])

def box_iou_calc(boxes1, boxes2):
    # https://github.com/pytorch/vision/blob/master/torchvision/ops/boxes.py
    """
    Return intersection-over-union (Jaccard index) of boxes.
    Both sets of boxes are expected to be in (x1, y1, x2, y2) format.
    Arguments:
        boxes1 (Array[N, 4])
        boxes2 (Array[M, 4])
    Returns:
        iou (Array[N, M]): the NxM matrix containing the pairwise
            IoU values for every element in boxes1 and boxes2
    This implementation is taken from the above link and changed so that it only uses numpy..
    """

    def box_area(box):
        # box = 4xn
        return (box[2] - box[0]) * (box[3] - box[1])

    area1 = box_area(boxes1.T)
    area2 = box_area(boxes2.T)

    lt = np.maximum(boxes1[:, None, :2], boxes2[:, :2])  # [N,M,2]
    rb = np.minimum(boxes1[:, None, 2:], boxes2[:, 2:])  # [N,M,2]

    inter = np.prod(np.clip(rb - lt, a_min=0, a_max=None), 2)
    return inter / (area1[:, None] + area2 - inter)  # iou = inter / (area1 + area2 - inter)


class ConfusionMatrix:
    def __init__(self, num_classes: int, CONF_THRESHOLD=0.3, IOU_THRESHOLD=0.5):
        self.matrix = np.zeros((num_classes + 1, num_classes + 1))
        self.num_classes = num_classes
        self.CONF_THRESHOLD = CONF_THRESHOLD
        self.IOU_THRESHOLD = IOU_THRESHOLD

    def process_batch(self, detections, labels: np.ndarray):
        """
        Return intersection-over-union (Jaccard index) of boxes.
        Both sets of boxes are expected to be in (x1, y1, x2, y2) format.
        Arguments:
            detections (Array[N, 6]), x1, y1, x2, y2, conf, class
            labels (Array[M, 5]), class, x1, y1, x2, y2
        Returns:
            None, updates confusion matrix accordingly
        """
        gt_classes = labels[:, 0].astype(np.int16)

        try:
            detections = detections[detections[:, 4] > self.CONF_THRESHOLD]
        except IndexError or TypeError:
            # detections are empty, end of process
            for i, label in enumerate(labels):
                gt_class = gt_classes[i]
                self.matrix[self.num_classes, gt_class] += 1
            return

        detection_classes = detections[:, 5].astype(np.int16)

        all_ious = box_iou_calc(labels[:, 1:], detections[:, :4])
        want_idx = np.where(all_ious > self.IOU_THRESHOLD)

        all_matches = [[want_idx[0][i], want_idx[1][i], all_ious[want_idx[0][i], want_idx[1][i]]]
                       for i in range(want_idx[0].shape[0])]

        all_matches = np.array(all_matches)
        if all_matches.shape[0] > 0:  # if there is match
            all_matches = all_matches[all_matches[:, 2].argsort()[::-1]]

            all_matches = all_matches[np.unique(all_matches[:, 1], return_index=True)[1]]

            all_matches = all_matches[all_matches[:, 2].argsort()[::-1]]

            all_matches = all_matches[np.unique(all_matches[:, 0], return_index=True)[1]]

        for i, label in enumerate(labels):
            gt_class = gt_classes[i]
            if all_matches.shape[0] > 0 and all_matches[all_matches[:, 0] == i].shape[0] == 1:
                detection_class = detection_classes[int(all_matches[all_matches[:, 0] == i, 1][0])]
                self.matrix[detection_class, gt_class] += 1
            else:
                self.matrix[self.num_classes, gt_class] += 1

        for i, detection in enumerate(detections):
            if not all_matches.shape[0] or ( all_matches.shape[0] and all_matches[all_matches[:, 1] == i].shape[0] == 0 ):
                detection_class = detection_classes[i]
                self.matrix[detection_class, self.num_classes] += 1

    def return_matrix(self):
        return self.matrix

    def print_matrix(self):
        for i in range(self.num_classes + 1):
            print(' '.join(map(str, self.matrix[i])))

# predictions=np.concatenate((pred_boxes[0][0],pred_boxes[1].T), axis=1)
# predictions=np.concatenate((predictions,pred_boxes[2].T), axis=1)
# test_boxes=np.concatenate((pred_boxes[2].T,pred_boxes[0][0]), axis=1)
# print(test_boxes)

predictions=np.concatenate((pred_boxes[0][0],pred_boxes[1].T), axis=1)
predictions=np.concatenate((predictions,pred_boxes[2].T), axis=1)

train_df_selection=df_extrain_main[['x_min', 'y_min','x_max','y_max','class','img_name']]# .to_numpy()

# classes=pd.DataFrame([{'class':'Platelets','label':0},{'class':'WBC','label':1},{'class':'RBC','label':2},{'class':'FBC','label':3}])
# join_df=pd.merge(train_df_selection,classes,on='class')
# join_df=join_df[['x_min', 'y_min','x_max','y_max','label']]
# join_df=join_df.astype('int64')
# join_df=join_df.to_numpy()

classes=pd.DataFrame([{'class':'Platelets','label':0},{'class':'WBC','label':1},{'class':'RBC','label':2},{'class':'FBC','label':3}])
selection=pd.DataFrame(selection,columns=['img_name'])
join_df=pd.merge(train_df_selection,classes,on='class')

join_df=pd.merge(join_df,selection,on='img_name')

join_df=join_df[['x_min', 'y_min','x_max','y_max','label']]
join_df=join_df.astype('int64')
join_df=join_df.to_numpy()

conf_mat = ConfusionMatrix(num_classes = 4, CONF_THRESHOLD = 0.4, IOU_THRESHOLD = 0.5)

print(conf_mat.print_matrix())

plt.imshow(conf_mat.return_matrix())
plt.xlabel('Original', fontsize=18)
plt.ylabel('Predicted', fontsize=16)
plt.colorbar()
plt.show()

"""<a id="1"></a>
# <p style="background-color:#000000;font-family:newtimeroman;color:#fff;font-size:120%;text-align:center;border-radius:20px 80px;"> Reference </p>

> ### [Fizyr Keras Retinanet](https://github.com/fizyr/keras-retinanet)
> ### [Great Barrier Reef API Tutorial](https://www.kaggle.com/sohier/great-barrier-reef-api-tutorial)
> ### [Reef- Starter Torch FasterRCNN Infer](https://www.kaggle.com/julian3833/reef-starter-torch-fasterrcnn-infer-lb-0-413)

"""